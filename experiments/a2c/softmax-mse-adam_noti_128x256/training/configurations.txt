_Configurations__run_size:
  end: 1581
  start: 0
_Configurations__run_size_ratio:
  end: 1.0
  start: 0.0
_from_column: true
a2c: true
activation: softmax
activation_last_layer: null
bars: 10
batch_size: 32
cnn: false
commission: 0.001
cpu_cores: 1
data: numpy array of size (1581, 25)
data_as_df: false
data_filter: Date<20200100
episodes: 1000
epsilon: 1.0
epsilon_decay: 0.99999
epsilon_end: 0.1
epsilon_min: 0.1
epsilon_start: 1.0
epsilon_steps: 900
gpu_cores: 0
ignore_columns: []
investment: 20000
join_columns:
- Date
join_columns_data: Dataframe with columns Index(['Date'], dtype='object') and size
  (1581, 1)
load_model: null
loss: mse
loss_critic: mse
memory_size: 32
mode: train
model_directory: D:/Thesis/bda-deep-rl/softmax-mse-adam/a2c-128-256-no-ti/training\models
n_stocks: 5
nn_layers:
- 128
- 256
optimizer: adam
output: D:/Thesis/bda-deep-rl/softmax-mse-adam/a2c-128-256-no-ti/training
prices: 'numpy array of size (5,)

  [ 3  8 13 18 23]'
random_memory_sampling: false
reward_directory: D:/Thesis/bda-deep-rl/softmax-mse-adam/a2c-128-256-no-ti/training\rewards
technical_indicators: false
train_directory: D:/Thesis/bda-deep-rl/softmax-mse-adam/a2c-128-256-no-ti/../../data/daily/
train_file: null
